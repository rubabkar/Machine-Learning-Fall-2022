{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e12cfd81",
   "metadata": {},
   "source": [
    "## CIFAR 10\n",
    "\n",
    "Classification (10 classes)\n",
    "\n",
    "CIFAR 10 consists of: 60,000 tiny 32 x 32 color RGB images ## MNIST is 28 x 28 and not RGB\n",
    "\n",
    "Labeled with Integer 1 to 10 classes \n",
    "\n",
    "Airplane (0), Car (1), etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc51414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score, accuracy_score, f1_score\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de25ea03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/cifar10data/'\n",
    "\n",
    "cifar10 = datasets.CIFAR10(data_path, train = True, download = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf969f0",
   "metadata": {},
   "source": [
    "### Here, we download the CIFAR 10 data into the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a07ce0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_val = datasets.CIFAR10(data_path, train = False, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8be627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = datasets.CIFAR10(data_path, train = True, download = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00a3a9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: data/cifar10data/\n",
      "    Split: Train\n"
     ]
    }
   ],
   "source": [
    "print(cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24138dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar10[99]\n",
    "\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "145bc177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c19e14bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AugMix', 'AutoAugment', 'AutoAugmentPolicy', 'CenterCrop', 'ColorJitter', 'Compose', 'ConvertImageDtype', 'FiveCrop', 'GaussianBlur', 'Grayscale', 'InterpolationMode', 'Lambda', 'LinearTransformation', 'Normalize', 'PILToTensor', 'Pad', 'RandAugment', 'RandomAdjustSharpness', 'RandomAffine', 'RandomApply', 'RandomAutocontrast', 'RandomChoice', 'RandomCrop', 'RandomEqualize', 'RandomErasing', 'RandomGrayscale', 'RandomHorizontalFlip', 'RandomInvert', 'RandomOrder', 'RandomPerspective', 'RandomPosterize', 'RandomResizedCrop', 'RandomRotation', 'RandomSolarize', 'RandomVerticalFlip', 'Resize', 'TenCrop', 'ToPILImage', 'ToTensor', 'TrivialAugmentWide', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_pil_constants', '_presets', 'autoaugment', 'functional', 'functional_pil', 'functional_tensor', 'transforms']\n"
     ]
    }
   ],
   "source": [
    "print( dir(transforms) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb320b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FCE95FA2280>\n",
      "\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(img)\n",
    "print(\"\")\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "img_t = to_tensor(img)\n",
    "\n",
    "print(img_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe4f7ac",
   "metadata": {},
   "source": [
    "### The 'transforms' attribute can be passed directly to the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49bab0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_cifar10 = datasets.CIFAR10(data_path, train = True, download = False, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42d75ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t, _ = tensor_cifar10[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07065c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2431, 0.1961, 0.1804,  ..., 0.6549, 0.7176, 0.5373],\n",
      "         [0.2471, 0.2157, 0.2039,  ..., 0.6392, 0.6706, 0.5686],\n",
      "         [0.2275, 0.2510, 0.2196,  ..., 0.6000, 0.5882, 0.4824],\n",
      "         ...,\n",
      "         [0.6745, 0.5608, 0.5098,  ..., 0.3686, 0.5529, 0.5451],\n",
      "         [0.7176, 0.5882, 0.3137,  ..., 0.3176, 0.5294, 0.5608],\n",
      "         [0.8196, 0.7137, 0.5451,  ..., 0.2314, 0.5098, 0.6627]],\n",
      "\n",
      "        [[0.2510, 0.1961, 0.1725,  ..., 0.6745, 0.7216, 0.5333],\n",
      "         [0.2549, 0.2078, 0.1961,  ..., 0.6627, 0.6824, 0.5725],\n",
      "         [0.2431, 0.2588, 0.2353,  ..., 0.6078, 0.6039, 0.5020],\n",
      "         ...,\n",
      "         [0.5294, 0.4314, 0.2196,  ..., 0.2941, 0.4235, 0.4118],\n",
      "         [0.5725, 0.4627, 0.2510,  ..., 0.2824, 0.4627, 0.4902],\n",
      "         [0.6824, 0.5922, 0.4275,  ..., 0.2118, 0.4667, 0.6118]],\n",
      "\n",
      "        [[0.1725, 0.1020, 0.0745,  ..., 0.2706, 0.2980, 0.2824],\n",
      "         [0.1451, 0.1020, 0.1059,  ..., 0.2392, 0.2941, 0.3020],\n",
      "         [0.1412, 0.1451, 0.1451,  ..., 0.2431, 0.2510, 0.2235],\n",
      "         ...,\n",
      "         [0.3882, 0.3294, 0.1647,  ..., 0.2196, 0.3373, 0.3176],\n",
      "         [0.4588, 0.3725, 0.1725,  ..., 0.2353, 0.3843, 0.4314],\n",
      "         [0.5647, 0.4824, 0.3255,  ..., 0.1843, 0.4353, 0.6275]]])\n"
     ]
    }
   ],
   "source": [
    "print(img_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf12a53",
   "metadata": {},
   "source": [
    "### This section depicts how to Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f72b2752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32, 50000])\n"
     ]
    }
   ],
   "source": [
    "imgs_s = [ img_t for img_t, _ in tensor_cifar10 ]\n",
    "imgs = torch.stack(imgs_s, dim = 3)\n",
    "\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b95dceba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4914, 0.4822, 0.4465])\n"
     ]
    }
   ],
   "source": [
    "view1 = imgs.view(3, -1).mean(dim = 1)\n",
    "print(view1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "555af658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2470, 0.2435, 0.2616])\n"
     ]
    }
   ],
   "source": [
    "view2 = imgs.view(3, -1).std(dim = 1)\n",
    "print(view2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "babfdc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_cifar10 = datasets.CIFAR10(data_path, train = True, download = False, \n",
    "                                       transform = transforms.Compose([\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize(view1, view2)\n",
    "                                         ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61bdba7",
   "metadata": {},
   "source": [
    "### Normalize and Converted to tensors for PyTorch - Train and Val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23a7196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_cifar10_val = datasets.CIFAR10(data_path, train = False, download = False, \n",
    "                                       transform = transforms.Compose([\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize(view1, view2)\n",
    "                                         ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af95ad16",
   "metadata": {},
   "source": [
    "### Now, we build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e46450e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0:0, 2:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f34af115",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'bird']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09571460",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar2 = [ (img, label_map[label]) for img, label in transformed_cifar10 if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "242a8d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar2_val = [ (img, label_map[label]) for img, label in transformed_cifar10_val if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202a81f",
   "metadata": {},
   "source": [
    "### This section depicts the Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fb56b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1158d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52f280b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "print(softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9eae00",
   "metadata": {},
   "source": [
    "### Convert Vectors from 32 x 32 x 3 to 1 x 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dff7f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = 2\n",
    "\n",
    "model_mlp = nn.Sequential(\n",
    "    \n",
    "    nn.Linear(3072, 512),\n",
    "    nn.Tanh(),\n",
    "    \n",
    "    nn.Linear(512, n_out),\n",
    "    nn.Softmax(dim = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c0d1213",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2DL = nn.Sequential(\n",
    "    nn.Linear(3072, 1024),\n",
    "    nn.ReLU(),        ## nn.GeLU()\n",
    "    \n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Linear(512, 128),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Linear(128, 2),\n",
    "    nn.LogSoftmax(dim = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a9ba36",
   "metadata": {},
   "source": [
    "### Negative Log Likelihood Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f97b8548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.NLLLoss()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ba144c",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95a01e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "867b843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model_fn = model_mlp\n",
    "model_fn = model_2DL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca012be",
   "metadata": {},
   "source": [
    "### The Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbfd0513",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2    #0.001\n",
    "n_epochs = 100\n",
    "\n",
    "\n",
    "## optimizer = optim.SGD(model_fn.parameters(), lr = learning_rate)\n",
    "optimizer = optim.Adam( model_fn.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef5d7a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5140, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5769, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6829, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6991, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7013, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6911, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6859, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7070, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7022, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6905, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6861, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6828, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6980, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6982, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6958, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7027, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6831, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6889, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6904, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6993, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6868, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7104, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6949, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6940, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6991, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6981, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6906, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6744, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6880, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6935, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7010, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6914, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6939, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6862, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6893, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6922, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6870, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6888, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6951, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7013, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6993, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6650, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6941, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6991, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6900, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7004, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6933, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6919, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6998, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6901, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7009, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6925, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6924, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6938, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6967, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6991, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7071, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6916, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6954, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6932, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6956, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6953, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range (n_epochs):\n",
    "    for imgs, labels in train_loader:       ## imgs [64x3x32x32]\n",
    "        \n",
    "        ## resize for network\n",
    "        batch_size = imgs.shape[0]\n",
    "        imgs_resized = imgs.view(batch_size, -1)    ## imgs_resized [64x3072]\n",
    "        \n",
    "        outputs = model_fn(imgs_resized)\n",
    "        \n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(loss)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15637b1e",
   "metadata": {},
   "source": [
    "## After training, now we test the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebd35040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader( cifar2_val, batch_size = 64, shuffle = False )\n",
    "\n",
    "temp_metric = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model_fn( imgs.view(batch_size, -1) )\n",
    "        \n",
    "        # print('Size of Matrix: ', outputs.shape)\n",
    "        # print(labels.shape)\n",
    "        \n",
    "        _ , pred = torch.max( outputs, dim = 1 )    ## dim = 1 causes a horizontal search, whereas dim = 0 causes a vertical search\n",
    "        \n",
    "        temp_metric = temp_metric + int( (pred == labels).sum() )\n",
    "        total = total + batch_size\n",
    "        \n",
    "print (temp_metric/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1be59d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats_percentage_train_test( algorithm_name, y_test, y_pred):    \n",
    "     print(\"------------------------------------------------------\")\n",
    "     print(\"------------------------------------------------------\")\n",
    "    \n",
    "     print(\"The Algorithm is: \", algorithm_name)\n",
    "        \n",
    "     print('Accuracy: %.2f' % accuracy_score(y_test,   y_pred) )\n",
    "     \n",
    "     confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "     print(\"Confusion matrix: \")\n",
    "     print(confmat)\n",
    "     print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "     print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred, average='weighted'))\n",
    "     print('F1-measure: %.3f' % f1_score(y_true=y_test, y_pred=y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "613fee87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "The Algorithm is:  2DL\n",
      "Accuracy: 0.44\n",
      "Confusion matrix: \n",
      "[[0 9]\n",
      " [0 7]]\n",
      "Precision: 0.191\n",
      "Recall: 0.438\n",
      "F1-measure: 0.266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ITS5200/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print_stats_percentage_train_test(\"2DL\", labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65da0530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITS5200",
   "language": "python",
   "name": "its5200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
